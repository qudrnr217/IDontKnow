# ê²°ì •ì™• ê¹€ëª¨ë¥´ Back-end

## ğŸŒ³ 1. ê°œë°œ í™˜ê²½
- *IDE*

  - IntelliJ IDEA Ultimate 2022.2.2
    
- *Node.js*
    
  - node-v14.17.0
    
- *Vue.js*
    
  - v2.7.10
    
- *JDK*
    
  - openjdk 11.0.16 2022-07-19

- *Apache Tomcat (ë‚´ì¥ ì•„íŒŒì¹˜ í†°ìº£ WAS)*
    
  - [org.apache.tomcat.embed](https://mvnrepository.com/artifact/org.apache.tomcat.embed)
    Â Â»Â [tomcat-embed-core](https://mvnrepository.com/artifact/org.apache.tomcat.embed/tomcat-embed-core) [v9.0.48]
    
- *ì›¹ì„œë²„ ë° ë¦¬ë²„ìŠ¤ í”„ë¡ì‹œ ì„œë²„*
    
  - nginx version: nginx/1.22.0 (Ubuntu)
  - docker pull nginx:stable-alpine
  - port: 80(HTTP), 443(HTTPS)
    
- *Docker*
    
  - Docker version 20.10.18, build b40c2f6
    
- *MySQL*
    
  - v8.0.30-1.el8
  - docker pull mysql:8.0
  - port: 3306
    
- *Jenkins*
    
  - v2.361.1
  - docker pull jenkins
  - port: 8080

### ğŸ§‘â€ğŸ’» ê¸°ìˆ  ìŠ¤íƒ
    
- **Spring Boot:** v2.5.2
- **RestAPI:** Spring REST Docs
- **Testing:** JUnit5, Mockito
- **Build Tool:** Gradle
- **Authorization:** Spring Security, JWT
- **DB access:** JPA, QueryDSL
- **Batch Job:** Quartz Scheduler
- **Email Send:** SMTP
## 2. Hadoop MapReduce ë¹…ë°ì´í„° ë¶„ì‚° ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤

### **1. ì „ì²˜ë¦¬**

  Raw Dataset: [ì—…ì¢… ëª©ì ì§€ë³„ ë°°ë‹¬ ì£¼ë¬¸ê±´ìˆ˜ ë°ì´í„°ì…‹](https://bdp.kt.co.kr/invoke/SOKBP2603/?goodsCode=KGUDSTNORDER)

  ![ë°ì´í„°ì…‹](../assets/images/1.png)

  *preprocess.py*  
  ```python
    import pandas
    
    xlsx = pandas.read_excel('./input.xlsx', sheet_name=1)
    print(xlsx.head())
    xlsx.to_csv('input.tsv', sep='\t', index=False, header=False)
  ```

### **2. ì „ì²˜ë¦¬ ê²°ê³¼ë¬¼**

![ì „ì²˜ë¦¬ ê²°ê³¼ë¬¼](../assets/images/2.png)

### **3. ë¶„ì‚°ì²˜ë¦¬**

- *í•˜ë‘¡ ëª…ë ¹ì–´*
    ```bash
    0. start-dfs.sh
    1. ant
    2. hdfs dfs -mkdir idontknow_wordcount
    3. hdfs dfs -put data/input.txt idontknow_wordcount
    4. hdfs dfs -rm -r idontknow_out
    5. hadoop jar ssafy.jar wordcount idontknow_wordcount idontknow_out
    6. hadoop dfs -cat idowntknow_out/part-r-0000 | more
    7. hadoop fs -get idontknow_out ~/result
    ```

- *build.xml*

    ```xml
    This XML file does not appear to have any style information associated with it. The document tree is shown below.
    <project name="Hadoop" default="package">
    <!--  Load all the default properties, and any the user wants     -->
    <!--  to contribute (without having to type -D or edit this file  -->
    <property file="${user.home}/build.properties"/>
    <property file="${basedir}/build.properties"/>
    <property name="build.encoding" value="UTF-8"/>
    <property name="lib.dir" value="/home/hadoop/hadoop/share/hadoop/"/>
    <property name="works.dir" value="${basedir}/src"/>
    <property name="build.dir" value="${basedir}/build"/>
    <property name="build.classes" value="${build.dir}/classes"/>
    <property name="build.sysclasspath" value="last"/>
    <property name="build.works" value="${build.dir}/works"/>
    <property name="javac.debug" value="on"/>
    <property name="javac.optimize" value="on"/>
    <property name="javac.deprecation" value="off"/>
    <property name="javac.version" value="1.8"/>
    <property name="javac.args" value=""/>
    <property name="javac.args.warnings" value=""/>
    <property name="javac.args.warnings" value="-Xlint:checked"/>
    <!--  the normal classpath  -->
    <path id="classpath">
    <fileset dir="${lib.dir}">
    <include name="**/*.jar"/>
    </fileset>
    </path>
    <!--  ======================================================  -->
    <!--  Stuff needed by all targets                             -->
    <!--  ======================================================  -->
    <target name="init">
    <mkdir dir="${build.dir}"/>
    <mkdir dir="${build.classes}"/>
    <mkdir dir="${build.works}"/>
    </target>
    <target name="compile-works" depends="init">
    <javac encoding="${build.encoding}" srcdir="${works.dir}" includes="**/*.java" destdir="${build.works}" debug="${javac.debug}" optimize="${javac.optimize}" target="${javac.version}" source="${javac.version}" deprecation="${javac.deprecation}">
    <compilerarg line="${javac.args} ${javac.args.warnings}"/>
    <classpath refid="classpath"/>
    </javac>
    </target>
    <!--  ==================================================================  -->
    <!--  Make the Hadoop work jar.                                           -->
    <!--  ==================================================================  -->
    <!--                                                                      -->
    <!--  ==================================================================  -->
    <target name="ssafy-works" depends="compile-works">
    <jar jarfile="${build.dir}/ssafy.jar" basedir="${build.works}">
    <manifest>
    <attribute name="Main-Class" value="ssafy/Driver"/>
    </manifest>
    </jar>
    </target>
    <!--  ==================================================================  -->
    <!--  D I S T R I B U T I O N                                             -->
    <!--  ==================================================================  -->
    <!--                                                                      -->
    <!--  ==================================================================  -->
    <target name="package" depends="ssafy-works">
    <copy file="${build.dir}/ssafy.jar" todir="../../../Desktop"/>
    </target>
    </project>
    ```
    
- *Driver.java*
    
    ```java
    package ssafy;
    
    import org.apache.hadoop.util.ProgramDriver;
    
    public class Driver {
    	public static void main(String[] args) {
    		int exitCode = -1;
    		ProgramDriver pgd = new ProgramDriver();
    		try {
    
    			pgd.addClass("wordcount", Wordcount.class, "A map/reduce program that counts pairs in the input files.");
    			pgd.driver(args);
    			exitCode = 0;
    		}
    		catch(Throwable e) {
    			e.printStackTrace();
    		}
    
    		System.exit(exitCode);
    	}
    }
    ```
    
- *Wordcount.java*
    
    ```java
    package ssafy;
    
    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.IntWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Job;
    import org.apache.hadoop.mapreduce.Mapper;
    import org.apache.hadoop.mapreduce.Reducer;
    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
    import org.apache.hadoop.util.GenericOptionsParser;
    
    import java.io.IOException;
    import java.util.HashMap;
    import java.util.Map;
    
    public class Wordcount {
    	/* 
    	Object, Text : input key-value pair type (always same (to get a line of input file))
    	Text, IntWritable : output key-value pair type
    	*/
    	public static class TokenizerMapper
    			extends Mapper<Object,Text,Text,IntWritable> {
    
    		// ìì¹˜êµ¬ => ID
    		private static Map<String, Integer> districtMap = new HashMap<>();
    		static {
    			districtMap.put("ê°•ë‚¨êµ¬", 1);
    			districtMap.put("ê°•ë™êµ¬", 2);
    			districtMap.put("ê°•ë¶êµ¬", 3);
    			districtMap.put("ê°•ì„œêµ¬", 4);
    			districtMap.put("ê´€ì•…êµ¬", 5);
    			districtMap.put("ê´‘ì§„êµ¬", 6);
    			districtMap.put("êµ¬ë¡œêµ¬", 7);
    			districtMap.put("ê¸ˆì²œêµ¬", 8);
    			districtMap.put("ë…¸ì›êµ¬", 9);
    			districtMap.put("ë„ë´‰êµ¬", 10);
    			districtMap.put("ë™ëŒ€ë¬¸êµ¬", 11);
    			districtMap.put("ë™ì‘êµ¬", 12);
    			districtMap.put("ë§ˆí¬êµ¬", 13);
    			districtMap.put("ì„œëŒ€ë¬¸êµ¬", 14);
    			districtMap.put("ì„œì´ˆêµ¬", 15);
    			districtMap.put("ì„±ë™êµ¬", 16);
    			districtMap.put("ì„±ë¶êµ¬", 17);
    			districtMap.put("ì†¡íŒŒêµ¬", 18);
    			districtMap.put("ì–‘ì²œêµ¬", 19);
    			districtMap.put("ì˜ë“±í¬êµ¬", 20);
    			districtMap.put("ìš©ì‚°êµ¬", 21);
    			districtMap.put("ì€í‰êµ¬", 22);
    			districtMap.put("ì¢…ë¡œêµ¬", 23);
    			districtMap.put("ì¤‘êµ¬", 24);
    			districtMap.put("ì¤‘ë‘êµ¬", 25);
    		}
    		// variable declairations
    		private IntWritable emitVal = new IntWritable();
    		private Text emitKey = new Text();
    
    		// map function (Context -> fixed parameter)
    		public void map(Object key, Text value, Context context)
    				throws IOException, InterruptedException {
    
    			StringBuilder commonKeyText = new StringBuilder();
    			// value.toString() : get a line
    			String line = value.toString();
    			String[] row = line.split("\t");
    			String sido = row[0];
    			// ì„œìš¸íŠ¹ë³„ì‹œê°€ ì•„ë‹ˆë©´ ê·¸ëŒ€ë¡œ ë¦¬í„´
    			if("ì„œìš¸íŠ¹ë³„ì‹œ".equals(sido)) {
    				int sigungu = districtMap.get(row[1]);
    				String time = row[3];
    				commonKeyText.append(sigungu).append(",").append(time).append(",");
    				for (int i = 13; i < row.length ; i++) {
    					StringBuilder keyText = new StringBuilder(commonKeyText);
    					keyText.append(i - 12);
    					emitVal.set(Integer.parseInt(row[i]));
    					emitKey.set(keyText.toString());
    					// emit a key-value pair
    					context.write(emitKey, emitVal);
    				}
    			}
    		}
    	}
    
    	/*
    	Text, IntWritable : input key type and the value type of input value list
    	Text, IntWritable : output key-value pair type
    	*/
    	public static class IntSumReducer
    			extends Reducer<Text,IntWritable,Text,IntWritable> {
    
    		// variables
    		private IntWritable result = new IntWritable();
    
    		// key : a disticnt word
    		// values :  Iterable type (data list)
    		public void reduce(Text key, Iterable<IntWritable> values, Context context) 
    				throws IOException, InterruptedException {
    
    			int sum = 0;
    			for ( IntWritable val : values ) {
    				sum += val.get();
    			}
    			result.set(sum);
    			context.write(key,result);
    		}
    	}
    
    	/* Main function */
    	public static void main(String[] args) throws Exception {
    		Configuration conf = new Configuration();
    		String[] otherArgs = new GenericOptionsParser(conf,args).getRemainingArgs();
    		if ( otherArgs.length != 2 ) {
    			System.err.println("Usage: <in> <out>");
    			System.exit(2);
    		}
    		Job job = new Job(conf,"word count");
    		job.setJarByClass(Wordcount.class);
    
    		// let hadoop know my map and reduce classes
    		job.setMapperClass(TokenizerMapper.class);
    		job.setReducerClass(IntSumReducer.class);
    
    		job.setOutputKeyClass(Text.class);
    		job.setOutputValueClass(IntWritable.class);
    
    		// set number of reduces
    		job.setNumReduceTasks(2);
    
    		// set input and output directories
    		FileInputFormat.addInputPath(job,new Path(otherArgs[0]));
    		FileOutputFormat.setOutputPath(job,new Path(otherArgs[1]));
    		System.exit(job.waitForCompletion(true) ? 0 : 1 );
    	}
    }
    ```
    

### **4. ë¶„ì‚°ì²˜ë¦¬ ê²°ê³¼ë¬¼**

1. *part-r-00000.txt*
    
    ```
    1,0,11	6
    1,0,13	4
    1,0,2	1
    1,0,4	37
    1,0,6	134
    1,0,8	41
    1,1,1	188
    1,1,10	84
    1,1,12	34
    1,1,14	29
    1,1,3	81
    1,1,5	59
    1,1,7	30
    1,1,9	2
    1,10,1	244
    1,10,10	498
    1,10,12	22
    1,10,14	17
    1,10,3	5
    1,10,5	0
    1,10,7	3
    1,10,9	9
    1,11,11	20
    1,11,13	71
    1,11,2	239
    1,11,4	74
    1,11,6	35
    ```
    
2.  *part-r-00001.txt*
    
    ```
    1,0,1	213
    1,0,10	106
    1,0,12	35
    1,0,14	32
    1,0,3	94
    1,0,5	102
    1,0,7	27
    1,0,9	6
    1,1,11	3
    1,1,13	3
    1,1,2	1
    1,1,4	35
    1,1,6	96
    1,1,8	48
    1,10,11	14
    1,10,13	13
    1,10,2	72
    1,10,4	22
    1,10,6	2
    1,10,8	14
    1,11,1	439
    1,11,10	468
    1,11,12	43
    1,11,14	29
    1,11,3	14
    1,11,5	22
    1,11,7	12
    1,11,9	74
    1,12,11	13
    ```
    

### **5. í›„ì²˜ë¦¬**
*postprocess.py*
```python
# 2. part-r-00000, part-r-00001 íŒŒì¼ì„ í•˜ë‚˜ì˜ csv íŒŒì¼ë¡œ í•©ì¹˜ê¸° (ì •ë ¬ í¬í•¨)
import csv

res = []
with open('part-r-00000', 'r') as f1, open('part-r-00001', 'r') as f2, open('result.csv', 'w', encoding='utf-8', newline='') as f3:
    rows = f1.readlines()
    for row in rows:
        key, value = row.rstrip().split('\t')
        row = list(map(int, key.split(',')))
        row.append(int(value))
        res.append(row)
    rows = f2.readlines()
    for row in rows:
        key, value = row.rstrip().split('\t')
        row = list(map(int, key.split(',')))
        row.append(int(value))
        res.append(row)
    res.sort()
    csv_writer = csv.writer(f3)
    csv_writer.writerow(["ìì¹˜êµ¬ ì½”ë“œ", "ì‹œê°„ëŒ€", "ë©”ë‰´ ì½”ë“œ", "ì´ ë°°ë‹¬ ê±´ìˆ˜"])
    csv_writer.writerows(res)
```
    

### **6. í›„ì²˜ë¦¬ ê²°ê³¼ë¬¼**
*output.csv*  
![í›„ì²˜ë¦¬ ê²°ê³¼ë¬¼](../assets/images/3.png)

### **7. ë¶„ì‚°ì²˜ë¦¬ ìµœì¢… ê²°ê³¼ë¬¼ DBì— ì €ì¥**
*insert.sql*
```sql
SET FOREIGN_KEY_CHECKS = 0;

USE  idontknow;

LOAD DATA INFILE 'C:\\ProgramData\\MySQL\\MySQL Server 8.0\\Uploads\\result.csv'
INTO TABLE data
FIELDS TERMINATED BY ','
ENCLOSED BY '"'
LINES TERMINATED BY '\n'
IGNORE 1 LINES
(@district_id, @`time`, @menu_id, @order_quantity)
SET `district_id` = @district_id,
    `time` = SEC_TO_TIME(@`time`*60*60),
    `menu_id` = @menu_id,
    `order_quantity` = @order_quantity;

SET FOREIGN_KEY_CHECKS = 1;
```

## ğŸ‘·â€â™€ï¸ 3. ë¹Œë“œ ë° DevOps ë©”ë‰´ì–¼

### DevOps ëª©í‘œ  

  **1. develop branchì— feat branchë¥¼ merge í•˜ì˜€ì„ë•Œ ë„ì»¤ ì´ë¯¸ì§€ ë¹Œë“œ ë° ì»¨í…Œì´ë„ˆ ë°°í¬**  
  **2. Jenkins, MySQL, Spring Boot Application + Tomcat, Vue.js + Nginxë¥¼ ëª¨ë‘ ì»¨í…Œì´ë„ˆí™”í•˜ì—¬ ë°°í¬**

### Jenkins ì„¤ì •

- Jenkins port: 8080
- ì•„ë˜ì™€ ê°™ì´ Front-end, Back-end CI/CD êµ¬ì¶•ì„ ìœ„í•œ ì•„ì´í…œ 2ê°œ ìƒì„±
  
![ì  í‚¨ìŠ¤ ëŒ€ì‹œë³´ë“œ](../assets/images/4.png)

- Jenkins Item êµ¬ì„±ì€ ë‹¤ìŒê³¼ ê°™ì´ ì„¤ì •
  - Frontend Itemì€ Backend Itemê³¼ `Filter merge request by label` ê³¼ `Branch Specifier (blank for 'any')`ë¥¼ ì œì™¸í•˜ê³  ë™ì¼
    
    ![GitLab ì—°ê²°](../assets/images/5.png)
    
    ![Trigger](../assets/images/6.png)
    
    ![Untitled](../assets/images/7.png)
    
### DevOps_ê¸°ë³¸ ì„¸íŒ…
- Jenkins, MySQL docker container ì‹¤í–‰
  - *docker-compose.mysql.yml*
    ```yml
      version: '3'
      services:
        mysql:
          image: mysql:8.0
          container_name: mysql
          ports:
            - 3306:3306 # HOST:CONTAINER
          environment:
            MYSQL_DATABASE: idontknow           
            MYSQL_ROOT_PASSWORD: admin@a208!@
            MYSQL_USER: develop
            MYSQL_PASSWORD: develop@a208!@
            TZ: Asia/Seoul
          command:
            - --character-set-server=utf8mb4
            - --collation-server=utf8mb4_unicode_ci
          volumes:
            - ./mysql/data:/var/lib/mysql
          networks:
            - idontknownetwork
      - 
      networks:
        idontknownetwork:
          external: true
    ```
  - *docker-compose.jenkins.yml*
      ```yml
        version: '3'
        services:
            jenkins:
                image: jenkins/jenkins:lts
                container_name: jenkins
                volumes:
                    - /var/run/docker.sock:/var/run/docker.sock
                    - /jenkins:/var/jenkins_home
                ports:
                    - "8080:8080"
                privileged: true
                user: root
        ```
- docker network ìƒì„±
  - `sudo docker network create idontknownetwork` : docker ì»¨í…Œì´ë„ˆê°„ì˜ ë„¤íŠ¸ì›Œí¬ ìƒì„± (`idontknownetwork`ë¼ëŠ” ì´ë¦„ì˜)
- Jenkins ì»¨í…Œì´ë„ˆ ì•ˆì—ì„œ docker ì‹¤í–‰í•˜ë©´ Hostì˜ ë„ì»¤ì™€ ì—°ê²°ë¨
- ì•„ë˜ì˜ Dockerfileë“¤ì„ backend/ ì™€ frontend/ì— ê°ê° í•˜ë‚˜ì”© ë„£ê³  nginx.confëŠ” /frontendì— ë„£ìŒ

### Docker_Frontend ëª…ë ¹ì–´
![execute sheel frontend](../assets/images/9.png)

```bash
# ë„ì»¤ ì‹œì‘ ì „, ê¸°ì¡´ì— ì‹¤í–‰ì¤‘ì¸ ë„ì»¤ë¥¼ ë©ˆì¶”ê³  ì œê±°í•˜ëŠ” ì‘ì—….
docker ps -f name=frontend -q | xargs --no-run-if-empty docker container stop

# ì»¨í…Œì´ë„ˆ ì œê±°
docker container ls -a -f name=frontend -q | xargs -r docker container rm

# frontend ë„ì»¤ ì´ë¯¸ì§€ ìƒì„±
docker build -t frontend:latest ./frontend

# frontend ë„ì»¤ ì»¨í…Œì´ë„ˆ ì‹¤í–‰ 
docker run -d --name frontend -p 80:80 -p 443:443 -v /etc/letsencrypt/:/etc/letsencrypt/ -v /etc/localtime:/etc/localtime:ro --network idontknownetwork frontend:latest
```
- `-v /etc/letsencrypt/:/etc/letsencrypt/` ë¡œ volumne mount ê²½ë¡œë¥¼ ì¡ì€ ì´ìœ : pemí‚¤ê°€ symlinkë¡œ ì €ì¥ë˜ì—ˆê¸° ë•Œë¬¸ì— letsencrpytê²½ë¡œë¶€í„° volume mount í•´ì•¼í•¨ !!! ğŸ˜„

### Docker_Backend ëª…ë ¹ì–´
![execute sheel backend](../assets/images/8.png)
```bash
# ë„ì»¤ ì‹œì‘ ì „, ê¸°ì¡´ì— ì‹¤í–‰ì¤‘ì¸ ë„ì»¤ë¥¼ ë©ˆì¶”ê³  ì œê±°í•˜ëŠ” ì‘ì—….
docker ps -f name=backend -q | xargs --no-run-if-empty docker container stop

# ì»¨í…Œì´ë„ˆ ì œê±°
docker container ls -a -f name=backend -q | xargs -r docker container rm

# backend ë„ì»¤ ì´ë¯¸ì§€ ìƒì„±
docker build -t backend:latest ./backend

# backend ë„ì»¤ ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -d --name backend -p 8081:8081 -v /home/ubuntu/properties/:/home/ubuntu/properties/ --network idontknownetwork backend:latest
```


### Docker_Frontend Dockerfile

```docker
FROM node:lts-alpine as build-stage
WORKDIR /frontend

COPY . .
RUN npm install
RUN npm run build

FROM nginx:stable-alpine as production-stage

RUN rm /etc/nginx/conf.d/default.conf
COPY ./nginx.conf /etc/nginx/conf.d/nginx.conf
COPY --from=build-stage ./frontend/dist /usr/share/nginx/html
EXPOSE 80 443
CMD ["nginx", "-g", "daemon off;"]
```



### Docker_Backend Dockerfile

- ì´ìŠˆ: classpath ì¸ì‹ ì•ˆë˜ì–´ application.ymlì„ ìŠ¤í”„ë§ì´ ì½ì§€ ëª»í–ˆìŒ â†’ í•´ê²°: application.yml ì„ ì»¨í…Œì´ë„ˆì— ì§ì ‘ ë³µì‚¬ í›„ ì‚¬ìš©í•¨

```docker
FROM openjdk:11
COPY /build/libs/api-0.0.1-SNAPSHOT.jar app.jar
COPY /src/main/resources/application.yml application.yml
EXPOSE 8081
ENTRYPOINT ["java", "-jar", "-Duser.timezone=Asia/Seoul", "-Dspring.profiles.active=prod", "-Dspring.config.location=/application.yml,/home/ubuntu/properties/application-db.yml", "app.jar"]
```

### Docker_Frontend nginx.conf

```bash
server {
        listen 80 default_server;
        listen [::]:80 default_server;

        server_name j7a208.p.ssafy.io;

        return 301 https://$server_name$request_uri;
}

server {
        listen 443 ssl;
        listen [::]:443 ssl;

        root /usr/share/nginx/html;
        index index.html index.htm;

        server_name j7a208.p.ssafy.io;

        ssl_certificate /etc/letsencrypt/live/j7a208.p.ssafy.io/fullchain.pem;
        ssl_certificate_key /etc/letsencrypt/live/j7a208.p.ssafy.io/privkey.pem;

        location / {
                try_files $uri $uri/ /index.html;
        }

        location /api {
                proxy_pass http://backend:8081;
                proxy_http_version 1.1;
                proxy_set_header Connection "";
                proxy_set_header Host $host;
                proxy_set_header X-Real-IP $remote_addr;
                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
                proxy_set_header X-Forwarded-Proto $scheme;
                proxy_set_header X-Forwarded-Host $host;
                proxy_set_header X-Forwarded-Port $server_port;
        }

        location /docs {
              proxy_pass http://backend:8081;
        }
}
```

- `http://backend:8081` ì˜ `backend`ëŠ” idontknow networkì— ì†í•œ backend container ì´ë¦„
  
### DevOps ê²°ê³¼
- Jenkins ë¹Œë“œ ê²°ê³¼
![ë°ë¸Œì˜µìŠ¤ ê²°ê³¼1](../assets/images/10.png)


- Mattermost ë¹Œë“œ ë° ë°°í¬ ì•Œë¦¼
![ë°ë¸Œì˜µìŠ¤ ê²°ê³¼2](../assets/images/11.png)
## ğŸ—ï¸ 4. í”„ë¡œì íŠ¸ êµ¬ì¡°ë„
```text
.
â””â”€â”€ src
    â”œâ”€â”€ docs
    â”‚   â””â”€â”€ asciidoc
    â”œâ”€â”€ main
    â”‚   â”œâ”€â”€ java
    â”‚   â”‚   â””â”€â”€ com
    â”‚   â”‚       â””â”€â”€ idk
    â”‚   â”‚           â””â”€â”€ api
    â”‚   â”‚               â”œâ”€â”€ common
    â”‚   â”‚               â”‚   â”œâ”€â”€ advice
    â”‚   â”‚               â”‚   â”œâ”€â”€ category
    â”‚   â”‚               â”‚   â”œâ”€â”€ entity
    â”‚   â”‚               â”‚   â””â”€â”€ exception
    â”‚   â”‚               â”œâ”€â”€ config
    â”‚   â”‚               â”‚   â””â”€â”€ jpa
    â”‚   â”‚               â”œâ”€â”€ districtcode
    â”‚   â”‚               â”‚   â”œâ”€â”€ domain
    â”‚   â”‚               â”‚   â”‚   â”œâ”€â”€ entity
    â”‚   â”‚               â”‚   â”‚   â””â”€â”€ repository
    â”‚   â”‚               â”‚   â””â”€â”€ exception
    â”‚   â”‚               â”œâ”€â”€ home
    â”‚   â”‚               â”‚   â”œâ”€â”€ config
    â”‚   â”‚               â”‚   â”œâ”€â”€ controller
    â”‚   â”‚               â”‚   â”œâ”€â”€ domain
    â”‚   â”‚               â”‚   â”‚   â”œâ”€â”€ entity
    â”‚   â”‚               â”‚   â”‚   â””â”€â”€ repository
    â”‚   â”‚               â”‚   â”œâ”€â”€ dto
    â”‚   â”‚               â”‚   â”œâ”€â”€ exception
    â”‚   â”‚               â”‚   â”œâ”€â”€ scheduler
    â”‚   â”‚               â”‚   â””â”€â”€ service
    â”‚   â”‚               â”œâ”€â”€ menucode
    â”‚   â”‚               â”‚   â””â”€â”€ domain
    â”‚   â”‚               â”‚       â”œâ”€â”€ entity
    â”‚   â”‚               â”‚       â””â”€â”€ repository
    â”‚   â”‚               â”œâ”€â”€ user
    â”‚   â”‚               â”‚   â”œâ”€â”€ controller
    â”‚   â”‚               â”‚   â”œâ”€â”€ domain
    â”‚   â”‚               â”‚   â”‚   â”œâ”€â”€ entity
    â”‚   â”‚               â”‚   â”‚   â””â”€â”€ repository
    â”‚   â”‚               â”‚   â”œâ”€â”€ dto
    â”‚   â”‚               â”‚   â”œâ”€â”€ exception
    â”‚   â”‚               â”‚   â”œâ”€â”€ security
    â”‚   â”‚               â”‚   â”‚   â”œâ”€â”€ token
    â”‚   â”‚               â”‚   â”‚   â””â”€â”€ userdetails
    â”‚   â”‚               â”‚   â””â”€â”€ service
    â”‚   â”‚               â””â”€â”€ vote
    â”‚   â”‚                   â”œâ”€â”€ controller
    â”‚   â”‚                   â”œâ”€â”€ domain
    â”‚   â”‚                   â”‚   â”œâ”€â”€ entity
    â”‚   â”‚                   â”‚   â””â”€â”€ repository
    â”‚   â”‚                   â”œâ”€â”€ dto
    â”‚   â”‚                   â”œâ”€â”€ exception
    â”‚   â”‚                   â””â”€â”€ service
    â”‚   â””â”€â”€ resources
    â”œâ”€â”€ rest
    â””â”€â”€ test
        â””â”€â”€ java
            â””â”€â”€ com
                â””â”€â”€ idk
                    â””â”€â”€ api
                        â””â”€â”€ api
                            â””â”€â”€ home
                                â””â”€â”€ controller
```